{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3178136",
   "metadata": {},
   "source": [
    "# **Speech Recognition**\n",
    "**Spoken Digits Dataset - https://github.com/Jakobovski/free-spoken-digit-dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72792c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "import librosa\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "import tqdm.auto as tqdm\n",
    "from torchsummary import summary\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e80d1399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device=\"cuda\"\n",
    "else:\n",
    "    device=\"cpu\"\n",
    "print(f\"Using {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccbd2fd",
   "metadata": {},
   "source": [
    "## **Data Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ffa169f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpokenDigiitsDataset(Dataset):\n",
    "    def __init__(self, audio_dir) -> None:\n",
    "        super(SpokenDigiitsDataset, self).__init__()\n",
    "        self.labels = []\n",
    "        self.audio_dir = audio_dir\n",
    "        self.sample_size = 22050\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(glob.glob(f'{self.audio_dir}/*.wav'))\n",
    "\n",
    "    def _mel_spectrogram(self, audio):\n",
    "        mel_spec = librosa.feature.melspectrogram(audio, sr=22050, n_fft=1024, hop_length=512, n_mels=64)\n",
    "        mel_spec = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "        return mel_spec\n",
    "\n",
    "    def _right_pad(self, signal):\n",
    "        if signal.shape[0] < self.sample_size:\n",
    "            missing_samples = self.sample_size - signal.shape[0]\n",
    "            last_dim_padding = (0, missing_samples)\n",
    "            signal = F.pad(torch.tensor(signal), last_dim_padding)\n",
    "            signal = signal.numpy()\n",
    "        return signal\n",
    "\n",
    "    def _truncate(self, signal):\n",
    "        if signal.shape[0] > self.sample_size:\n",
    "            signal = signal[:self.sample_size]\n",
    "        return signal\n",
    "\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        audio_path = glob.glob(f'{self.audio_dir}/*.wav')[index]\n",
    "        audio, sr = librosa.load(audio_path)\n",
    "        audio = librosa.resample(audio, sr, 22050)\n",
    "        audio = librosa.util.normalize(audio)\n",
    "        audio = self._truncate(audio)\n",
    "        audio = self._right_pad(audio)\n",
    "        audio = self._mel_spectrogram(audio)\n",
    "        audio = torch.tensor(audio, dtype=torch.float32, device=device)\n",
    "        \n",
    "        label = audio_path.split('/')[-1].split('_')[1]\n",
    "        if label not in self.labels:\n",
    "            num = len(self.labels)\n",
    "            self.labels.append(label)\n",
    "        else:\n",
    "            num = self.labels.index(label)\n",
    "        \n",
    "        label = num\n",
    "        #label = torch.tensor(label, dtype=torch.int, device=device)\n",
    "        return audio, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8926118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-60.7656, -39.9687, -33.3869,  ..., -80.0000, -80.0000, -80.0000],\n",
      "        [-54.5900, -21.0452, -15.9190,  ..., -80.0000, -80.0000, -80.0000],\n",
      "        [-51.0485, -16.5689, -19.2177,  ..., -80.0000, -80.0000, -80.0000],\n",
      "        ...,\n",
      "        [-69.5762, -80.0000, -80.0000,  ..., -80.0000, -80.0000, -80.0000],\n",
      "        [-69.7309, -80.0000, -80.0000,  ..., -80.0000, -80.0000, -80.0000],\n",
      "        [-69.8274, -80.0000, -80.0000,  ..., -80.0000, -80.0000, -80.0000]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "datapath = \"./data/spoken-digits/\"\n",
    "dataset = SpokenDigiitsDataset(datapath)\n",
    "dataten, label = dataset[0]\n",
    "print(dataten)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32eaa20b",
   "metadata": {},
   "source": [
    "## **PyTorch Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79c94c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple lstm model for speec recognition\n",
    "class SpeechRecognitionModel(nn.Module):\n",
    "    def __init__(self, input_size=44, hidden_size=64, num_layers=2, num_classes=6):\n",
    "        super(SpeechRecognitionModel, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size*64, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = out.reshape(out.shape[0], -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac3b16c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 44])\n"
     ]
    }
   ],
   "source": [
    "signal_shape = dataset[0][0].shape\n",
    "print(signal_shape)\n",
    "model = SpeechRecognitionModel().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145f5483",
   "metadata": {},
   "source": [
    "## **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72022574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, loss_fn, optimizer, device, epochs):\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        for batch in tqdm.tqdm(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            x, y = batch\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            y_hat = model(x)\n",
    "            loss = loss_fn(y_hat, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch {i} Loss: {loss.item()}\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "404611bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_dataloader = DataLoader(dataset, batch_size)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "device=\"cuda\"\n",
    "epochs=10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76dac1e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79efd0eaf7604f45b31c6c32e72ae3cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Loss: 0.2653054893016815\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a6e1126825247d782b04f1c99cf49d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.21847163140773773\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "475c7e976fdf4fd6a15a233978632819",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 0.12817074358463287\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c5da416e60942929e5230b397b5c01e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 0.15764343738555908\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e929acfe7fc44b0b0c464c38a6a1382",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 0.13405515253543854\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e62126a8bb84248a36639a8eb75eec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 0.12168055772781372\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a94372cad36f40bdb7fbae0beb561896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Loss: 0.08614056557416916\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86e12ef3e1a14e6fb6b5e2e550d05a91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Loss: 0.07463821023702621\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "185582b4ae48460bbd8dc452af7b2ce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Loss: 0.06552808731794357\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d392e8bb12764926a9281b9aaa1698db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/47 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Loss: 0.04016384854912758\n",
      "Trained Feed Forward Network\n"
     ]
    }
   ],
   "source": [
    "model = train(model, train_dataloader, loss_fn, optimizer, device, epochs)\n",
    "\n",
    "torch.save(model.state_dict(), \"Speech-Recognition.pth\")\n",
    "print(\"Trained Feed Forward Network\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53daae9e",
   "metadata": {},
   "source": [
    "## **Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35b5506a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_dataloader = DataLoader(dataset, batch_size)\n",
    "model.eval()\n",
    "predictions = []\n",
    "for _, (X, y) in enumerate(train_dataloader):\n",
    "    X = X.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = model(X)\n",
    "        for i in range(len(pred)):\n",
    "            arr = pred[i].to(\"cpu\")\n",
    "            predictions.append(np.argmax(arr))\n",
    "\n",
    "    del pred\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c08a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "i=0\n",
    "print(len(predictions))\n",
    "for _, (_, y) in enumerate(train_dataloader):\n",
    "    y = y.to(\"cpu\").numpy()\n",
    "    for j in range(len(y)):\n",
    "        if(predictions[i] == y[j]):\n",
    "            print(predictions[i], y[j])\n",
    "            count+=1\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "427b012b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9646666666666667\n"
     ]
    }
   ],
   "source": [
    "print(count/len(predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "6c8b17707a87e86f58fc8e0e03557a8732d4849cf558d6d13f0acd878e2598f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
