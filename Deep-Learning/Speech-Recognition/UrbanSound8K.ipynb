{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49f7e476",
   "metadata": {},
   "source": [
    "# Sound Classification - UrbanSound4K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31b3ce40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb6fb68",
   "metadata": {},
   "source": [
    "## Dataset & Pre Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cd8cc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UrbanSoundDataset(Dataset):\n",
    "    def __init__(self, annotations_file, audio_dir, sample_rate, sample_size):\n",
    "        self.annotations = pd.read_csv(annotations_file)\n",
    "        self.audio_dir = audio_dir\n",
    "        self.sample_rate = sample_rate\n",
    "        self.sample_size = sample_size\n",
    "        self.device = self._is_cuda()\n",
    "     \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        audio_path = self._get_audio_sample_path(index)\n",
    "        label = self._get_audio_sample_label(index)\n",
    "        '''\n",
    "        signal = audio signal, sr = sample rate\n",
    "        signal -> [num_of_channels, samples] \n",
    "        '''\n",
    "        signal, sr = torchaudio.load(audio_path)\n",
    "        signal = signal.to(self.device)\n",
    "        signal = self._resample(signal, sr)\n",
    "        signal = self._mix_down(signal)\n",
    "        signal = self._right_pad(signal)\n",
    "        signal = self._truncate(signal)\n",
    "        signal = self._mel_spectrogram(signal)\n",
    "        return signal, label\n",
    "    \n",
    "    def _is_cuda(self):\n",
    "        if torch.cuda.is_available():\n",
    "            device=\"cuda\"\n",
    "        else:\n",
    "            device=\"cpu\"\n",
    "        print(f\"Using {device}\")\n",
    "        return device\n",
    "    \n",
    "    def _right_pad(self, signal):\n",
    "        if signal.shape[1] < self.sample_size:\n",
    "            missing_samples = self.sample_size - signal.shape[1]\n",
    "            last_dim_padding = (0, missing_samples)\n",
    "            signal = F.pad(signal, last_dim_padding)\n",
    "        return signal\n",
    "    \n",
    "    def _truncate(self, signal):\n",
    "        if signal.shape[1] > self.sample_size:\n",
    "            signal = signal[:, :self.sample_size]\n",
    "        return signal\n",
    "    \n",
    "    def _resample(self, signal, sr):\n",
    "        if sr != self.sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.sample_rate).to(self.device)\n",
    "            signal = resampler(signal)\n",
    "        return signal\n",
    "    \n",
    "    def _mel_spectrogram(self, signal):\n",
    "        mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "                            sample_rate=sample_rate, \n",
    "                            n_fft=1024,\n",
    "                            hop_length=512,\n",
    "                            n_mels=64,\n",
    "                            ).to(self.device)\n",
    "        signal = mel_spectrogram(signal)\n",
    "        return signal\n",
    "        \n",
    "    def _mix_down(self, signal):\n",
    "        if(signal.shape[0] != 1):\n",
    "            signal = torch.mean(signal, dim=0, keepdim=True)\n",
    "        return signal\n",
    "    \n",
    "    def _get_audio_sample_path(self, index):\n",
    "        fold = f\"fold{self.annotations.iloc[index, 5]}\"\n",
    "        path = os.path.join(self.audio_dir, fold, self.annotations.iloc[index, 0])\n",
    "        return path\n",
    "    \n",
    "    def _get_audio_sample_label(self, index):\n",
    "        return self.annotations.iloc[index, 6]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "600e54ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_file = \"/home/mnk/MNK/Mega/MegNav/Deep-Learning/Speech-Recognition/UrbanSound8K/metadata/UrbanSound8K.csv\"\n",
    "audio_dir = \"/home/mnk/MNK/Mega/MegNav/Deep-Learning/Speech-Recognition/UrbanSound8K/audio/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a33c1f9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "sample_rate = 22000\n",
    "sample_size = 22000\n",
    "data = UrbanSoundDataset(annotations_file, audio_dir, sample_rate, sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbe8ba28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[8.1120e-04, 2.2000e-04, 9.0258e-04,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [2.4777e-03, 1.5648e-03, 4.2762e-04,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [2.9734e-03, 6.0683e-03, 4.1591e-03,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         ...,\n",
      "         [1.7172e-04, 5.9824e-02, 7.9713e-01,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [2.0983e-04, 1.7792e-02, 4.1559e-01,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [3.0722e-04, 1.4679e-02, 3.0088e-01,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00]]], device='cuda:0')\n",
      "3\n",
      "torch.Size([1, 64, 43])\n"
     ]
    }
   ],
   "source": [
    "signal, label = data[0]\n",
    "\n",
    "print(signal)\n",
    "print(label)\n",
    "print(signal.shape)\n",
    "signal_shape = signal.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7527fb75",
   "metadata": {},
   "source": [
    "## Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aff2228e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, filter):\n",
    "        super(CNN, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            #Layer-1\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,\n",
    "                out_channels=filter,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2                \n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            \n",
    "            #Layer-2\n",
    "            nn.Conv2d(\n",
    "                in_channels=filter,\n",
    "                out_channels=filter*2,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2                \n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            \n",
    "            #Layer-3\n",
    "            nn.Conv2d(\n",
    "                in_channels=filter*2,\n",
    "                out_channels=filter*4,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2                \n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            \n",
    "            #layer-4\n",
    "            nn.Conv2d(\n",
    "                in_channels=filter*4,\n",
    "                out_channels=filter*8,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2                \n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128*5*4, 10),\n",
    "            nn.Softmax(dim=1),    \n",
    "        )\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.output = self.network(input)\n",
    "        \n",
    "        return self.output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3b545fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 66, 45]             160\n",
      "              ReLU-2           [-1, 16, 66, 45]               0\n",
      "         MaxPool2d-3           [-1, 16, 33, 22]               0\n",
      "            Conv2d-4           [-1, 32, 35, 24]           4,640\n",
      "              ReLU-5           [-1, 32, 35, 24]               0\n",
      "         MaxPool2d-6           [-1, 32, 17, 12]               0\n",
      "            Conv2d-7           [-1, 64, 19, 14]          18,496\n",
      "              ReLU-8           [-1, 64, 19, 14]               0\n",
      "         MaxPool2d-9             [-1, 64, 9, 7]               0\n",
      "           Conv2d-10           [-1, 128, 11, 9]          73,856\n",
      "             ReLU-11           [-1, 128, 11, 9]               0\n",
      "        MaxPool2d-12            [-1, 128, 5, 4]               0\n",
      "          Flatten-13                 [-1, 2560]               0\n",
      "           Linear-14                   [-1, 10]          25,610\n",
      "          Softmax-15                   [-1, 10]               0\n",
      "================================================================\n",
      "Total params: 122,762\n",
      "Trainable params: 122,762\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.80\n",
      "Params size (MB): 0.47\n",
      "Estimated Total Size (MB): 2.28\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = CNN(filter=16).to(\"cuda\")\n",
    "summary(model, signal_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f196ae6",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad075bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, loss_fn, optimizer, device, epochs):\n",
    "    for i in range(epochs):\n",
    "        for j, (X, y) in enumerate(train_loader):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            predictions = model(X)\n",
    "            loss = loss_fn(predictions, y)\n",
    "            \n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        print(f\"Epoch {i} : Loss {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce9eb68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : Loss 2.203418254852295\n",
      "Epoch 1 : Loss 2.175096273422241\n",
      "Epoch 2 : Loss 2.0296285152435303\n",
      "Epoch 3 : Loss 2.030651569366455\n",
      "Epoch 4 : Loss 2.2335026264190674\n",
      "Epoch 5 : Loss 2.1404218673706055\n",
      "Epoch 6 : Loss 2.2797720432281494\n",
      "Epoch 7 : Loss 2.27921462059021\n",
      "Epoch 8 : Loss 2.3166377544403076\n",
      "Epoch 9 : Loss 2.316761016845703\n",
      "Trained Feed Forward Network\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "train_dataloader = DataLoader(data, batch_size)\n",
    "\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "device=\"cuda\"\n",
    "epochs=10\n",
    "train(model, train_dataloader, loss_fn, optimizer, device, epochs)\n",
    "\n",
    "torch.save(model.state_dict(), \"sound_classification.pth\")\n",
    "print(\"Trained Feed Forward Network\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908741ab",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b89f801e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 |  6% | 97% |\n"
     ]
    }
   ],
   "source": [
    "from GPUtil import showUtilization as gpu_usage\n",
    "gpu_usage()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9f0402b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_dataloader = DataLoader(data, batch_size)\n",
    "model.eval()\n",
    "predictions = []\n",
    "for _, (X, y) in enumerate(train_dataloader):\n",
    "    X = X.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = model(X)\n",
    "        for i in range(len(pred)):\n",
    "            arr = pred[i].to(\"cpu\")\n",
    "            predictions.append(np.argmax(arr))\n",
    "\n",
    "    del pred\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb8e0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "i=0\n",
    "print(len(predictions))\n",
    "for _, (_, y) in enumerate(train_dataloader):\n",
    "    y = y.to(\"cpu\").numpy()\n",
    "    for j in range(len(y)):\n",
    "        if(predictions[i] == y[j]):\n",
    "            print(predictions[i], y[j])\n",
    "            count+=1\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7ac686d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2201099404489235\n"
     ]
    }
   ],
   "source": [
    "print(count/len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff7844a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
